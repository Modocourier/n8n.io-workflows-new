================================================================================
WORKFLOW: MCP Server with AI Agent as a Tool Context Reducer
================================================================================

üìã BASIC INFORMATION
------------------------------
ID: 4475
Name: MCP Server with AI Agent as a Tool Context Reducer
Total Views: 668
Created At: 2025-05-29T05:17:43.176Z
Purchase URL: None

üë§ AUTHOR INFORMATION
------------------------------
Name: William Lettieri
Username: fibonacci
Verified: ‚ùå No
Bio: Accountant, Developer, Dreamer
Links: https://williamlettieri.com

üìù DESCRIPTION
------------------------------
Overview

Transform your LLM into a powerful GitHub automation specialist with this n8n workflow template. In a world where multiple MCP servers can overwhelm LLMs with context, this streamlined solution provides a dedicated GitHub Agent that handles all GitHub API operations through a single, specialized tool.

When you need GitHub operations like creating repositories, managing issues, or handling pull requests, your LLM can make one simple call to the GitHub Agent. This agent specializes exclusively in GitHub MCP server operations, offloading all contextual complexity and providing clean, efficient GitHub automation.

‚ú® Features

Single MCP Server Trigger** - One tool and one parameter to handle all GitHub API interactions  
Specialized GitHub Agent** - Dedicated AI agent with direct GitHub MCP Server connection
Self-Executing Workflow** - "When Executed by Another Workflow" trigger enables seamless workflow chaining
Scalable Architecture** - Ready to integrate with unlimited GitHub tools and operations
Context Optimization** - Reduces LLM token usage by delegating GitHub complexity to a specialized agent
Flexible Request Processing** - Handles any GitHub operation through natural language requests

üéØ Use Cases

Repository Management** - Create, clone, and manage repositories programmatically
Issue Tracking** - Automate issue creation, updates, and management workflows  
Pull Request Automation - Streamline code review and merge processes
GitHub Actions Integration** - Trigger and monitor CI/CD workflows
Team Collaboration** - Automate notifications and team management tasks
Documentation Updates** - Automatically update README files and documentation

üèóÔ∏è Workflow Architecture

Node Breakdown:
MCP Server Trigger - Receives requests with GitHub operation parameters
Set GitHub Username - Configures GitHub user context for API calls  
OpenAI Chat Model - Powers the intelligent GitHub agent with contextual understanding
Simple Memory - Maintains conversation context and operation history
GitHub AI Agent - Specialized Tools Agent with direct GitHub MCP Server access

[MCP Server Trigger] ‚Üí [Set GitHub Username] ‚Üí [GitHub AI Agent]
                                ‚Üì
[OpenAI Chat Model] ‚Üê [Simple Memory] ‚Üê [GitHub API Operations]

üìã Requirements

Essential Prerequisites:
‚úÖ OpenAI API Key - For AI Agent and Chat Model functionality
‚úÖ GitHub Username Configuration - Edit the "Set GitHub Username" node with your GitHub username for API calls
‚úÖ n8n Version - Compatible with n8n 2024+ releases
‚úÖ MCP Server Setup - Existing GitHub MCP server configuration

Recommended Setup:
GitHub Personal Access Token with appropriate permissions
Basic understanding of n8n workflow configuration
Familiarity with GitHub API operations

üöÄ Setup Instructions

Step 1: Import and Configure
Import the workflow template into your n8n instance
Navigate to the Set GitHub Username node
Replace the placeholder with your actual GitHub username

Step 2: API Keys Setup
Configure your OpenAI API key in the Chat Model node
Ensure your GitHub credentials are properly configured in n8n
Test the connection to verify API access

Step 3: MCP Server Integration  
Connect your existing GitHub MCP server to the workflow
Verify the MCP Server Trigger is properly configured
Test with a simple GitHub operation (e.g., "List my repositories")

Step 4: Deploy and Test
Activate the workflow in your n8n instance
Test with various GitHub operations to ensure functionality
Monitor execution logs for any configuration issues

üîß Customization Options

Agent Behavior
Modify the Chat Model prompt** to adjust agent personality and response style
Configure memory settings** to control conversation context retention
Adjust timeout settings** for long-running GitHub operations

GitHub Operations
Extend supported operations** by adding new GitHub API endpoints
Configure repository filters** to limit scope of operations
Set up notification preferences** for important GitHub events

Integration Points
Webhook triggers** for real-time GitHub event processing
Scheduled operations** for regular repository maintenance
Cross-workflow triggers** for complex automation chains

üí° Pro Tips

Start Simple**: Begin with basic operations like repository listing before attempting complex workflows
Monitor Token Usage**: The specialized agent approach significantly reduces OpenAI API costs
Batch Operations**: Group related GitHub operations in single requests for efficiency
Error Handling**: The agent provides detailed error messages for troubleshooting

ü§ù Support and Community

Documentation**: Official n8n Documentation
Community Forum**: n8n Community
Issues & Contributions**: Feel free to suggest improvements or report issues

üìÑ License

This workflow template is provided under the MIT License. You're free to use, modify, and redistribute with attribution.

Created by: William Lettieri
Version: 1.0  
Last Updated: May 28, 2025  
Compatibility: n8n 2024+

üîß NODES USED
------------------------------
‚Ä¢ AI Agent - Categories: AI, Langchain
‚Ä¢ OpenAI Chat Model - Categories: AI, Langchain
‚Ä¢ Simple Memory - Categories: AI, Langchain
‚Ä¢ Call n8n Workflow Tool - Categories: AI, Langchain
‚Ä¢ MCP Client Tool - Categories: AI, Langchain

Total Nodes: 5

üìä RAW DATA (JSON)
------------------------------
{
  "id": 4475,
  "name": "MCP Server with AI Agent as a Tool Context Reducer",
  "totalViews": 668,
  "purchaseUrl": null,
  "user": {
    "id": 92562,
    "name": "William Lettieri",
    "username": "fibonacci",
    "bio": "Accountant, Developer, Dreamer",
    "verified": false,
    "links": "[\"https://williamlettieri.com\"]",
    "avatar": "https://gravatar.com/avatar/edfd4883bf16cdb3b81007a6a6131adf4da5357cc67771aff7ef5e2b12ef4579?r=pg&d=retro&size=200"
  },
  "description": "Overview\n\nTransform your LLM into a powerful GitHub automation specialist with this n8n workflow template. In a world where multiple MCP servers can overwhelm LLMs with context, this streamlined solution provides a dedicated GitHub Agent that handles all GitHub API operations through a single, specialized tool.\n\nWhen you need GitHub operations like creating repositories, managing issues, or handling pull requests, your LLM can make one simple call to the GitHub Agent. This agent specializes exclusively in GitHub MCP server operations, offloading all contextual complexity and providing clean, efficient GitHub automation.\n\n‚ú® Features\n\nSingle MCP Server Trigger** - One tool and one parameter to handle all GitHub API interactions  \nSpecialized GitHub Agent** - Dedicated AI agent with direct GitHub MCP Server connection\nSelf-Executing Workflow** - \"When Executed by Another Workflow\" trigger enables seamless workflow chaining\nScalable Architecture** - Ready to integrate with unlimited GitHub tools and operations\nContext Optimization** - Reduces LLM token usage by delegating GitHub complexity to a specialized agent\nFlexible Request Processing** - Handles any GitHub operation through natural language requests\n\nüéØ Use Cases\n\nRepository Management** - Create, clone, and manage repositories programmatically\nIssue Tracking** - Automate issue creation, updates, and management workflows  \nPull Request Automation - Streamline code review and merge processes\nGitHub Actions Integration** - Trigger and monitor CI/CD workflows\nTeam Collaboration** - Automate notifications and team management tasks\nDocumentation Updates** - Automatically update README files and documentation\n\nüèóÔ∏è Workflow Architecture\n\nNode Breakdown:\nMCP Server Trigger - Receives requests with GitHub operation parameters\nSet GitHub Username - Configures GitHub user context for API calls  \nOpenAI Chat Model - Powers the intelligent GitHub agent with contextual understanding\nSimple Memory - Maintains conversation context and operation history\nGitHub AI Agent - Specialized Tools Agent with direct GitHub MCP Server access\n\n[MCP Server Trigger] ‚Üí [Set GitHub Username] ‚Üí [GitHub AI Agent]\n                                ‚Üì\n[OpenAI Chat Model] ‚Üê [Simple Memory] ‚Üê [GitHub API Operations]\n\nüìã Requirements\n\nEssential Prerequisites:\n‚úÖ OpenAI API Key - For AI Agent and Chat Model functionality\n‚úÖ GitHub Username Configuration - Edit the \"Set GitHub Username\" node with your GitHub username for API calls\n‚úÖ n8n Version - Compatible with n8n 2024+ releases\n‚úÖ MCP Server Setup - Existing GitHub MCP server configuration\n\nRecommended Setup:\nGitHub Personal Access Token with appropriate permissions\nBasic understanding of n8n workflow configuration\nFamiliarity with GitHub API operations\n\nüöÄ Setup Instructions\n\nStep 1: Import and Configure\nImport the workflow template into your n8n instance\nNavigate to the Set GitHub Username node\nReplace the placeholder with your actual GitHub username\n\nStep 2: API Keys Setup\nConfigure your OpenAI API key in the Chat Model node\nEnsure your GitHub credentials are properly configured in n8n\nTest the connection to verify API access\n\nStep 3: MCP Server Integration  \nConnect your existing GitHub MCP server to the workflow\nVerify the MCP Server Trigger is properly configured\nTest with a simple GitHub operation (e.g., \"List my repositories\")\n\nStep 4: Deploy and Test\nActivate the workflow in your n8n instance\nTest with various GitHub operations to ensure functionality\nMonitor execution logs for any configuration issues\n\nüîß Customization Options\n\nAgent Behavior\nModify the Chat Model prompt** to adjust agent personality and response style\nConfigure memory settings** to control conversation context retention\nAdjust timeout settings** for long-running GitHub operations\n\nGitHub Operations\nExtend supported operations** by adding new GitHub API endpoints\nConfigure repository filters** to limit scope of operations\nSet up notification preferences** for important GitHub events\n\nIntegration Points\nWebhook triggers** for real-time GitHub event processing\nScheduled operations** for regular repository maintenance\nCross-workflow triggers** for complex automation chains\n\nüí° Pro Tips\n\nStart Simple**: Begin with basic operations like repository listing before attempting complex workflows\nMonitor Token Usage**: The specialized agent approach significantly reduces OpenAI API costs\nBatch Operations**: Group related GitHub operations in single requests for efficiency\nError Handling**: The agent provides detailed error messages for troubleshooting\n\nü§ù Support and Community\n\nDocumentation**: Official n8n Documentation\nCommunity Forum**: n8n Community\nIssues & Contributions**: Feel free to suggest improvements or report issues\n\nüìÑ License\n\nThis workflow template is provided under the MIT License. You're free to use, modify, and redistribute with attribution.\n\nCreated by: William Lettieri\nVersion: 1.0  \nLast Updated: May 28, 2025  \nCompatibility: n8n 2024+",
  "createdAt": "2025-05-29T05:17:43.176Z",
  "nodes": [
    {
      "id": 1119,
      "icon": "fa:robot",
      "name": "@n8n/n8n-nodes-langchain.agent",
      "codex": {
        "data": {
          "alias": [
            "LangChain",
            "Chat",
            "Conversational",
            "Plan and Execute",
            "ReAct",
            "Tools"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Agents",
              "Root Nodes"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "AI Agent",
        "color": "#404040"
      },
      "iconData": {
        "icon": "robot",
        "type": "icon"
      },
      "displayName": "AI Agent",
      "typeVersion": 2,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1153,
      "icon": "file:openAiLight.svg",
      "name": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models",
              "Root Nodes"
            ],
            "Language Models": [
              "Chat Models (Recommended)"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "OpenAI Chat Model"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,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"
      },
      "displayName": "OpenAI Chat Model",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1163,
      "icon": "fa:database",
      "name": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Memory"
            ],
            "Memory": [
              "For beginners"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Simple Memory"
      },
      "iconData": {
        "icon": "database",
        "type": "icon"
      },
      "displayName": "Simple Memory",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1205,
      "icon": "fa:network-wired",
      "name": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolworkflow/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Tools"
            ],
            "Tools": [
              "Recommended Tools"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Call n8n Workflow Tool"
      },
      "iconData": {
        "icon": "network-wired",
        "type": "icon"
      },
      "displayName": "Call n8n Workflow Tool",
      "typeVersion": 2,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1292,
      "icon": "file:../mcp.svg",
      "name": "@n8n/n8n-nodes-langchain.mcpClientTool",
      "codex": {
        "data": {
          "alias": [
            "Model Context Protocol",
            "MCP Client"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolmcp/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Model Context Protocol",
              "Tools"
            ]
          }
        }
      },
      "group": "[\"output\"]",
      "defaults": {
        "name": "MCP Client"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTgwIiBoZWlnaHQ9IjE4MCIgdmlld0JveD0iMCAwIDE5NSAxOTUiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+Cgk8ZyBzdHJva2U9IiMwMDAiIHN0cm9rZS13aWR0aD0iMTIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+CgkJPHBhdGggZD0iTTI1IDk3Ljg1MjhMOTIuODgyMyAyOS45NzA2QzEwMi4yNTUgMjAuNTk4IDExNy40NTEgMjAuNTk4IDEyNi44MjMgMjkuOTcwNlYyOS45NzA2QzEzNi4xOTYgMzkuMzQzMSAxMzYuMTk2IDU0LjUzOTEgMTI2LjgyMyA2My45MTE3TDc1LjU1ODEgMTE1LjE3NyIvPgoJCTxwYXRoIGQ9Ik03Ni4yNjUzIDExNC40N0wxMjYuODIzIDYzLjkxMTdDMTM2LjE5NiA1NC41MzkxIDE1MS4zOTIgNTQuNTM5MSAxNjAuNzY1IDYzLjkxMTdMMTYxLjExOCA2NC4yNjUyQzE3MC40OTEgNzMuNjM3OCAxNzAuNDkxIDg4LjgzMzggMTYxLjExOCA5OC4yMDYzTDk5LjcyNDggMTU5LjZDOTYuNjAwNiAxNjIuNzI0IDk2LjYwMDYgMTY3Ljc4OSA5OS43MjQ4IDE3MC45MTNMMTEyLjMzMSAxODMuNTIiLz4KCQk8cGF0aCBkPSJNMTA5Ljg1MyA0Ni45NDExTDU5LjY0ODIgOTcuMTQ1N0M1MC4yNzU3IDEwNi41MTggNTAuMjc1NyAxMjEuNzE0IDU5LjY0ODIgMTMxLjA4N1YxMzEuMDg3QzY5LjAyMDggMTQwLjQ1OSA4NC4yMTY4IDE0MC40NTkgOTMuNTg5NCAxMzEuMDg3TDE0My43OTQgODAuODgyMiIvPgoJPC9nPgo8L3N2Zz4K"
      },
      "displayName": "MCP Client Tool",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    }
  ]
}