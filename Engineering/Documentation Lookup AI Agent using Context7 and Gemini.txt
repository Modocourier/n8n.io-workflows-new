================================================================================
WORKFLOW: Documentation Lookup AI Agent using Context7 and Gemini
================================================================================

üìã BASIC INFORMATION
------------------------------
ID: 4547
Name: Documentation Lookup AI Agent using Context7 and Gemini
Total Views: 355
Created At: 2025-06-01T03:32:24.786Z
Purchase URL: None

üë§ AUTHOR INFORMATION
------------------------------
Name: Jez
Username: jez
Verified: ‚úÖ Yes
Bio: I'm passionate about combining traditional web development with AI and automation to create impactful online solutions. As Founder & CEO of Jezweb, an award-winning Newcastle, Australia agency, I lead a 30+ person team delivering high-quality web design, development, and hosting services to over 3000 clients.
Links: https://www.jezweb.com.au

üìù DESCRIPTION
------------------------------
This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n. 

This workflow demonstrates how to build and expose a sophisticated n8n AI Agent as a single, callable tool using the Multi-Agent Collaboration Protocol (MCP). It allows external clients or other AI systems to easily query software library documentation via Context7, without needing to manage the underlying tool orchestration or complex conversational logic.

Core Idea:
Instead of building complex agentic loops on the client-side (e.g., in Python, a VS Code extension, or another AI development environment), this workflow offloads the entire agent's reasoning and tool-use process to n8n. The client simply sends a natural language query (like "How do I use Flexbox in Tailwind CSS?") to an SSE endpoint, and the n8n agent handles the rest.

Key Features & How It Works:

Public MCP Endpoint:
    The main workflow uses the Context7 MCP Server Trigger node to create an SSE endpoint. This makes the agent accessible to any MCP-compatible client.
    The path for the endpoint is kept long and random for basic 'security by obscurity'.
Tool Workflow as an Interface:
    A Tool Workflow node (named call_context7_ai_agent in this example) is connected to the MCP Server Trigger. This node defines the single "tool" that external clients will see and call.
Dedicated AI Agent Sub-Workflow:
    The call_context7_ai_agent tool invokes a separate sub-workflow which contains the actual AI logic.
    This sub-workflow starts with a Context7 Workflow Start node to receive the user's query.
    A Context7 AI Agent node (using Google Gemini in this example) is the brain, equipped with:
        A system prompt to guide its behavior.
        Simple Memory to retain context for each execution (using {{ $execution.id }} as the session key).
        Two specialized Context7 MCP client tools:
            context7-resolve-library-id: To convert library names (e.g., 'Next.js') into Context7-specific IDs.
            context7-get-library-docs: To fetch documentation using the resolved ID, with options for specific topics and token limits.
Seamless Tool Use: The AI Agent autonomously decides when and how to use the resolve-library-id and get-library-docs tools based on the user's query, handling the multi-step process internally.

Benefits of This Approach:

Simplified Client Integration:** Clients interact with a single, powerful tool, sending a simple query.
Reduced Client-Side Token Consumption:** The detailed prompts, tool descriptions, and conversational turns are managed server-side by n8n, saving tokens on the client (especially useful if the client is another LLM).
Centralized Agent Management:** Update your agent's capabilities, tools, or LLM model within n8n without any changes needed on the client side.
Modularity for Agentic Systems:** Perfect for building complex, multi-agent systems where this n8n workflow can act as a specialized "expert" agent callable by others (e.g., from environments like Smithery).
Cost-Effective:** By using a potentially less expensive model (like Gemini Flash) for the agent's orchestration and leveraging the free tier or efficient pricing of services like Context7, you can build powerful solutions economically.

Use Cases:

Providing an intelligent documentation lookup service for coding assistants or IDE extensions.
Creating specialized AI "micro-agents" that can be consumed by larger AI applications.
Building internal knowledge base query systems accessible via a simple API-like interface.

Setup:

Ensure you have the necessary n8n credentials for Google Gemini (or your chosen LLM) and the Context7 MCP client tools.
The Path in the Context7 MCP Server Trigger node should be unique and secure.
Clients connect to the "Production URL" (SSE endpoint) provided by the trigger node.

This workflow is a great example of how n8n can serve as a powerful backend for building and deploying modular AI agents.

I've made a video to try and explain this a bit too https://www.youtube.com/watch?v=dudvmyp7Pyg

üîß NODES USED
------------------------------
‚Ä¢ AI Agent - Categories: AI, Langchain
‚Ä¢ Simple Memory - Categories: AI, Langchain
‚Ä¢ Call n8n Workflow Tool - Categories: AI, Langchain
‚Ä¢ Google Gemini Chat Model - Categories: AI, Langchain

Total Nodes: 4

üìä RAW DATA (JSON)
------------------------------
{
  "id": 4547,
  "name": "Documentation Lookup AI Agent using Context7 and Gemini",
  "totalViews": 355,
  "purchaseUrl": null,
  "user": {
    "id": 95044,
    "name": "Jez",
    "username": "jez",
    "bio": "I'm passionate about combining traditional web development with AI and automation to create impactful online solutions. As Founder & CEO of Jezweb, an award-winning Newcastle, Australia agency, I lead a 30+ person team delivering high-quality web design, development, and hosting services to over 3000 clients.",
    "verified": true,
    "links": "[\"https://www.jezweb.com.au\"]",
    "avatar": "https://gravatar.com/avatar/5f6cbb73f1a61b0bea48b1fac47a94e693ca6f6bc2359eff9c7e68159698efc6?r=pg&d=retro&size=200"
  },
  "description": "This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n. \n\nThis workflow demonstrates how to build and expose a sophisticated n8n AI Agent as a single, callable tool using the Multi-Agent Collaboration Protocol (MCP). It allows external clients or other AI systems to easily query software library documentation via Context7, without needing to manage the underlying tool orchestration or complex conversational logic.\n\nCore Idea:\nInstead of building complex agentic loops on the client-side (e.g., in Python, a VS Code extension, or another AI development environment), this workflow offloads the entire agent's reasoning and tool-use process to n8n. The client simply sends a natural language query (like \"How do I use Flexbox in Tailwind CSS?\") to an SSE endpoint, and the n8n agent handles the rest.\n\nKey Features & How It Works:\n\nPublic MCP Endpoint:\n    The main workflow uses the Context7 MCP Server Trigger node to create an SSE endpoint. This makes the agent accessible to any MCP-compatible client.\n    The path for the endpoint is kept long and random for basic 'security by obscurity'.\nTool Workflow as an Interface:\n    A Tool Workflow node (named call_context7_ai_agent in this example) is connected to the MCP Server Trigger. This node defines the single \"tool\" that external clients will see and call.\nDedicated AI Agent Sub-Workflow:\n    The call_context7_ai_agent tool invokes a separate sub-workflow which contains the actual AI logic.\n    This sub-workflow starts with a Context7 Workflow Start node to receive the user's query.\n    A Context7 AI Agent node (using Google Gemini in this example) is the brain, equipped with:\n        A system prompt to guide its behavior.\n        Simple Memory to retain context for each execution (using {{ $execution.id }} as the session key).\n        Two specialized Context7 MCP client tools:\n            context7-resolve-library-id: To convert library names (e.g., 'Next.js') into Context7-specific IDs.\n            context7-get-library-docs: To fetch documentation using the resolved ID, with options for specific topics and token limits.\nSeamless Tool Use: The AI Agent autonomously decides when and how to use the resolve-library-id and get-library-docs tools based on the user's query, handling the multi-step process internally.\n\nBenefits of This Approach:\n\nSimplified Client Integration:** Clients interact with a single, powerful tool, sending a simple query.\nReduced Client-Side Token Consumption:** The detailed prompts, tool descriptions, and conversational turns are managed server-side by n8n, saving tokens on the client (especially useful if the client is another LLM).\nCentralized Agent Management:** Update your agent's capabilities, tools, or LLM model within n8n without any changes needed on the client side.\nModularity for Agentic Systems:** Perfect for building complex, multi-agent systems where this n8n workflow can act as a specialized \"expert\" agent callable by others (e.g., from environments like Smithery).\nCost-Effective:** By using a potentially less expensive model (like Gemini Flash) for the agent's orchestration and leveraging the free tier or efficient pricing of services like Context7, you can build powerful solutions economically.\n\nUse Cases:\n\nProviding an intelligent documentation lookup service for coding assistants or IDE extensions.\nCreating specialized AI \"micro-agents\" that can be consumed by larger AI applications.\nBuilding internal knowledge base query systems accessible via a simple API-like interface.\n\nSetup:\n\nEnsure you have the necessary n8n credentials for Google Gemini (or your chosen LLM) and the Context7 MCP client tools.\nThe Path in the Context7 MCP Server Trigger node should be unique and secure.\nClients connect to the \"Production URL\" (SSE endpoint) provided by the trigger node.\n\nThis workflow is a great example of how n8n can serve as a powerful backend for building and deploying modular AI agents.\n\nI've made a video to try and explain this a bit too https://www.youtube.com/watch?v=dudvmyp7Pyg",
  "createdAt": "2025-06-01T03:32:24.786Z",
  "nodes": [
    {
      "id": 1119,
      "icon": "fa:robot",
      "name": "@n8n/n8n-nodes-langchain.agent",
      "codex": {
        "data": {
          "alias": [
            "LangChain",
            "Chat",
            "Conversational",
            "Plan and Execute",
            "ReAct",
            "Tools"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Agents",
              "Root Nodes"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "AI Agent",
        "color": "#404040"
      },
      "iconData": {
        "icon": "robot",
        "type": "icon"
      },
      "displayName": "AI Agent",
      "typeVersion": 2,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1163,
      "icon": "fa:database",
      "name": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Memory"
            ],
            "Memory": [
              "For beginners"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Simple Memory"
      },
      "iconData": {
        "icon": "database",
        "type": "icon"
      },
      "displayName": "Simple Memory",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1205,
      "icon": "fa:network-wired",
      "name": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolworkflow/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Tools"
            ],
            "Tools": [
              "Recommended Tools"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Call n8n Workflow Tool"
      },
      "iconData": {
        "icon": "network-wired",
        "type": "icon"
      },
      "displayName": "Call n8n Workflow Tool",
      "typeVersion": 2,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1262,
      "icon": "file:google.svg",
      "name": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models",
              "Root Nodes"
            ],
            "Language Models": [
              "Chat Models (Recommended)"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Google Gemini Chat Model"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDggNDgiPjxkZWZzPjxwYXRoIGlkPSJhIiBkPSJNNDQuNSAyMEgyNHY4LjVoMTEuOEMzNC43IDMzLjkgMzAuMSAzNyAyNCAzN2MtNy4yIDAtMTMtNS44LTEzLTEzczUuOC0xMyAxMy0xM2MzLjEgMCA1LjkgMS4xIDguMSAyLjlsNi40LTYuNEMzNC42IDQuMSAyOS42IDIgMjQgMiAxMS44IDIgMiAxMS44IDIgMjRzOS44IDIyIDIyIDIyYzExIDAgMjEtOCAyMS0yMiAwLTEuMy0uMi0yLjctLjUtNCIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBmaWxsPSIjRkJCQzA1IiBkPSJNMCAzN1YxMWwxNyAxM3oiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiNFQTQzMzUiIGQ9Im0wIDExIDE3IDEzIDctNi4xTDQ4IDE0VjBIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiMzNEE4NTMiIGQ9Im0wIDM3IDMwLTIzIDcuOSAxTDQ4IDB2NDhIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiM0Mjg1RjQiIGQ9Ik00OCA0OCAxNyAyNGwtNC0zIDM1LTEweiIgY2xpcC1wYXRoPSJ1cmwoI2IpIi8+PC9zdmc+"
      },
      "displayName": "Google Gemini Chat Model",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    }
  ]
}